{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data update for 5 minutes. Moving to the next URL.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m     52\u001b[0m     driver\u001b[38;5;241m.\u001b[39mexecute_script(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindow.scrollBy(0, 500);\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 53\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Extract page content (HTML)\u001b[39;00m\n\u001b[1;32m     56\u001b[0m page_html \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mpage_source\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time # Import tqdm for the progress bar\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# Setup Selenium WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Define the list of URLs and the page limit per URL\n",
    "urls = [\n",
    "    \"https://www.tokopedia.com/search?st=&q=hoodie&srp_component_id=02.01.00.00&srp_page_id=&srp_page_title=&navsource=\",\n",
    "    \"https://www.tokopedia.com/search?st=&q=keyboard&srp_component_id=02.01.00.00&srp_page_id=&srp_page_title=&navsource=\",\n",
    "    \"https://www.tokopedia.com/search?st=&q=mac&srp_component_id=02.01.00.00&srp_page_id=&srp_page_title=&navsource=\",\n",
    "    \"https://www.tokopedia.com/search?st=&q=keycaps&srp_component_id=02.01.00.00&srp_page_id=&srp_page_title=&navsource=\",\n",
    "    \"https://www.tokopedia.com/search?st=&q=laptop%20gaming&srp_component_id=02.01.00.00&srp_page_id=&srp_page_title=&navsource=\",\n",
    "    \"https://www.tokopedia.com/search?st=&q=switch&srp_component_id=02.01.00.00&srp_page_id=&srp_page_title=&navsource=\",\n",
    "    \"https://www.tokopedia.com/search?st=&q=ps5&srp_component_id=02.01.00.00&srp_page_id=&srp_page_title=&navsource=\",\n",
    "    \"https://www.tokopedia.com/search?st=&q=iphone&srp_component_id=02.01.00.00&srp_page_id=&srp_page_title=&navsource=\"\n",
    "]\n",
    "pages_per_url = 150\n",
    "data_limit = 15000\n",
    "output_file = 'tokopedia_data.csv'\n",
    "timeout_minutes = 5  # Set the timeout period (5 minutes)\n",
    "\n",
    "# Helper function to handle missing elements\n",
    "def get_text_or_na(element):\n",
    "    return element.get_text().strip() if element else \"N/A\"\n",
    "\n",
    "\n",
    "total_pages = pages_per_url * len(urls)\n",
    "\n",
    "# Function to check if more than 5 minutes have passed since last update\n",
    "def has_timeout_exceeded(last_update_time):\n",
    "    return (datetime.datetime.now() - last_update_time).total_seconds() > timeout_minutes * 60\n",
    "\n",
    "# Loop through each URL\n",
    "for base_url in urls:\n",
    "    last_update_time = datetime.datetime.now()  # Initialize the last update time\n",
    "    for current_page in range(1, pages_per_url + 1):\n",
    "        # Construct the full URL for the current page\n",
    "        url = f\"{base_url}&page={current_page}\"\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for the page to load\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Scroll down multiple times to load all products in smaller increments\n",
    "        for _ in range(10):\n",
    "            driver.execute_script(\"window.scrollBy(0, 500);\")\n",
    "            time.sleep(3)\n",
    "\n",
    "        # Extract page content (HTML)\n",
    "        page_html = driver.page_source\n",
    "        soup = BeautifulSoup(page_html, \"html.parser\")\n",
    "\n",
    "        # Find all product boxes on the current page\n",
    "        product_boxes = soup.find_all('div', {\"class\": \"css-5wh65g\"})\n",
    "\n",
    "        # Create lists to store data for this page\n",
    "        product_names = []\n",
    "        product_prices = []\n",
    "        sellers = []\n",
    "        store_locations = []\n",
    "        sold_quantities = []\n",
    "        ratings = []\n",
    "\n",
    "        # Extract product details using the helper function\n",
    "        for product in product_boxes:\n",
    "            name = product.find('span', {\"class\": \"OWkG6oHwAppMn1hIBsC3pQ==\"})\n",
    "            price = product.find('div', {\"class\": \"_8cR53N0JqdRc+mQCckhS0g==\"})\n",
    "            seller = product.find('span', {\"class\": \"X6c-fdwuofj6zGvLKVUaNQ== -9tiTbQgmU1vCjykywQqvA== flip\"})\n",
    "            location = product.find('span', {\"class\": \"-9tiTbQgmU1vCjykywQqvA== flip\"})\n",
    "            sold = product.find('span', {\"class\": \"eLOomHl6J3IWAcdRU8M08A==\"})\n",
    "            rating = product.find('span', {\"class\": \"nBBbPk9MrELbIUbobepKbQ==\"})\n",
    "\n",
    "            # Append the extracted data using the helper function\n",
    "            product_names.append(get_text_or_na(name))\n",
    "            product_prices.append(get_text_or_na(price))\n",
    "            sellers.append(get_text_or_na(seller))\n",
    "            store_locations.append(get_text_or_na(location))\n",
    "            sold_quantities.append(get_text_or_na(sold))\n",
    "            ratings.append(get_text_or_na(rating))\n",
    "\n",
    "        # Create a DataFrame with the current page's data\n",
    "        page_data = pd.DataFrame({\n",
    "            'Product': product_names,\n",
    "            'Price': product_prices,\n",
    "            'Seller': sellers,\n",
    "            'Location': store_locations,\n",
    "            'Sold': sold_quantities,\n",
    "            'Rating': ratings\n",
    "        })\n",
    "\n",
    "        # Drop rows where all values are \"N/A\"\n",
    "        page_data.replace(\"N/A\", pd.NA, inplace=True)\n",
    "        page_data.dropna(how='all', inplace=True)\n",
    "\n",
    "        # Check if new data was found\n",
    "        if not page_data.empty:  # Only save if the DataFrame is not empty\n",
    "            # Save the data to a CSV file, append if the file already exists\n",
    "            page_data.to_csv(output_file, mode='a', header=not os.path.exists(output_file), index=False)\n",
    "            last_update_time = datetime.datetime.now()  # Reset the timer\n",
    "        # Check if the data limit has been reached\n",
    "        if len(product_names) >= data_limit:\n",
    "            print(f\"Data limit reached: {len(product_names)} products scraped.\")\n",
    "            break\n",
    "\n",
    "        # Check if timeout exceeded (if no new data in the last 5 minutes)\n",
    "        if has_timeout_exceeded(last_update_time):\n",
    "            print(f\"No data update for {timeout_minutes} minutes. Moving to the next URL.\")\n",
    "            break  # Break the page loop and move to the next URL\n",
    "\n",
    "# Close the WebDriver after scraping\n",
    "driver.quit()\n",
    "\n",
    "print(f\"Data has been progressively saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
